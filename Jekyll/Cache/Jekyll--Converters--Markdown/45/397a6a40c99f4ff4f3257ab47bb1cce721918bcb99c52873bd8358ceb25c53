I"$<blockquote>
  <p>📌통계 101 X 데이터 분석 <strong>Chapter 12-13</strong></p>
</blockquote>

<h3 id="차원-축소-dimension-reduction">차원 축소 Dimension Reduction</h3>

<p>데이터의 특징을 유지하면서 분석이나 결과 해석에 도움을 주면서, 변수의 수를 줄이는 것을 <strong>차원 축소</strong>라 한다.</p>

<blockquote>
  <p>❗변수의 수를 줄이는 이유</p>

  <ol>
    <li>고차원 데이터 해석의 어려움이 있다.</li>
    <li>다중회귀분석에서는 설명변수끼리 강한 상관이 있는 상황을 다중공선성이 있다고 하며, 회귀 계수 추정이 불안정해지는 문제가 발생한다.</li>
    <li>차원의 저주</li>
  </ol>
</blockquote>

<ul>
  <li>회귀분석에서 변수가 많다는 것은 그 만큼 파라미터 수가 많아지는 것을 의미한다.</li>
  <li>표본크기 n이 충분하지 않은 상황이라면, 회귀계수를 올바르게 추정할 수 없는 문제가 생긴다.</li>
</ul>

<h2 id="주성분-분석">주성분 분석</h2>

<p><strong>PCA, Principal Component Analysis</strong></p>

<p>주성분 분석에서는 새로운 축을 설정하고, 그 축 위의 값으로 데이터를 새롭게 바라본다.</p>

<p>💡 <strong>새로운 축은 데이터의 퍼짐이 가장 커지는 방향</strong>으로!</p>

<ul>
  <li>첫 번째 새로운 축을 PC1(제1주성분)</li>
  <li>PC1과 <strong>수직 방향</strong>이고 데이터 퍼짐이 가장 커지는 방향으로, 두 번째 축인 PC2(제2주성분)를 설정
    <ul>
      <li>PC2에는 PC1으로 나타낼 수 없는 나머지 소량의 정보가 포함되어 있다.</li>
    </ul>
  </li>
</ul>

<p>변수의 수가 M일 때, <strong>주성분의 개수는 최대 M</strong>이다.
각 주성분이 가진 정보(분산)의 비율을 <strong>기여율</strong>이라 하고, ‘제1부터 제k주성분까지 전체 정보의 몇 %가 포함되는지’를 <strong>누적기여율</strong></p>

<p>새수직이라는 성질을 가지기에 새로운 축의 변수와는 무상관.
각 주성분이 어떤 축인지를 조사하기 위해 각 주성분의 값과 원래 각 변수의 상관관계를 계산.</p>

<h3 id="통계학과-기계학습의-차이">통계학과 기계학습의 차이</h3>

<table>
  <thead>
    <tr>
      <th> </th>
      <th>통계학</th>
      <th>기계학습</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>데이터 크기</td>
      <td>작은 표본 크기를 가진 데이터를 대상으로</td>
      <td>대량의 데이터를 대상으로</td>
    </tr>
    <tr>
      <td>경향</td>
      <td>설명이나 해석을 중요시</td>
      <td>예측을 중요시</td>
    </tr>
  </tbody>
</table>

<p><strong>통계학</strong>의 선형회귀에서는 절편과 기울기를 가진 <strong>단순한</strong> 모형을 이용하며, 기울기는 설명변수 1개만이 달라졌을 때 반응변수가 어떻게 변화하는지를 나타내므로 쉽게 해석 가능하다.</p>

<p><strong>기계학습</strong> 모형은 <strong>복잡</strong>하기(비선형이며 파라미터가 많기) 때문에, 보통 모형을 해석하기 어렵다. 복잡한 모형은 단순한 모형보다 표현력이 강하며, 복잡한 현상에 대한 예측 성능이 높다.</p>

<h2 id="비지도-학습">비지도 학습</h2>

<p><strong>정답 데이터가 없으며</strong>, 데이터의 배후에 있는 구조를 올바르게 추출하려는 목적으로 사용된다. 비슷한 데이터 집합을 발견하는 <strong><em>군집분석</em></strong>과 고차원 데이터를 적은 수의 차원으로 줄이는 <strong><em>차원 축소</em></strong>가 대표적이다.</p>

<h3 id="군집분석-cluster-analysis">군집분석 Cluster Analysis</h3>

<p>각 데이터가 어떤 군집에 속하는지를 구하는 방법</p>

<ul>
  <li>데이터 간 유사도를 측정하고자 데이터 사이의 거리를 계산하지만, 거리의 정의는 다양하기에 분석하는 사람이 직접 지정해야 한다.
    <ul>
      <li>결과에는 분석하는 사람의 자의성이 개입된다는 사실에 주의해야 한다.</li>
    </ul>
  </li>
</ul>

<h4 id="k-평균-k-means">k-평균 k-means</h4>

<p>기본적인 군집화 방법으로, 몇 개의 군집으로 나눌 것인지 군집 개수 k를 정하며 시작한다.
각 데이터에 무작위로 군집을 할당하고, 각 군집의 중심 위치(데이터의 평균)를 구한다. 그 이후, 데이터마다 가장 근접한 군집을 할당하고 중심 위치를 다시 계산한다.</p>

<p>→ 이 과정을 반복해나가다가, 군집 할당이 바뀌지 않거나 할당의 변화량이 일정 이하일 때 계산을 멈추고, <strong>각 데이터에 대한 군집 할당</strong>을 얻는다.</p>

<h4 id="계층적-군집화-hierarchical-clustering">계층적 군집화 Hierarchical Clustering</h4>

<p>모든 데이터가 다른 군집에 속하는 상태에서 시작하여, 가장 거리가 가까운 군집끼리 순서대로 합쳐 나간다.</p>

<p><img src="/public/img/241110/HClustering.png" alt="Hierarchical Clustering" /></p>

<p><strong>Dendrogram 덴드로그램</strong>은 군집 개수는 미리 정해지지 않으며, 어느 단계에서 바라보느냐에 따라 달라진다.</p>

<p><strong>t-SNE(t-distributed Stochastic Neighbor Embedding)</strong>는 복잡한 비선형관계에도 적용할 수 있는 방법이다.</p>

<h2 id="지도-학습">지도 학습</h2>

<p>특정 x값일 때, y가 어떤 값이 되는지에 관한 데이터를 바탕으로 그 관계를 학습해 나가는 방법이다.</p>

<blockquote>
  <p>📌 <strong>학습</strong>은 모형에서 얻을 수 있는 출력을 가능한 한 실제 y에 가깝도록 하고자 모형의 파라미터를 update하는 것.</p>
</blockquote>

<p>계산 방법은 모형의 출력과 실제 y의 차이를 손실함수로 정식화하고, 이를 최소화하는 파라미터를 구하는 것.</p>

<blockquote>
  <p>❗반응변수 y가 양적 변수일 때는 <strong>회귀</strong>, 질적 변수일 때는 <strong>분류</strong>라 한다.</p>
</blockquote>

<h4 id="예측">예측</h4>

<p>학습에 사용한 데이터(훈련 데이터)를 설명 및 예측하는 것이 아니라, 동일 조건에서 얻을 수 있는 미지의 데이터(검증 데이터)에 대해 설명 변수 x로 반응변수 y를 예측하는 것이다.</p>

<h4 id="교차검증-cross-validation">교차검증 cross validation</h4>

<p>동일 조건에서 얻은 미지의 데이터를 얼마나 잘 예측할 수 있는가를 검증하기 위해 실제로 얻은 데이터를 둘로 분할한다.
한 쪽 데이터를 사용해 학습을 진행하고, 나머지 한 쪽 데이터는 예측이 얼마나 좋은지 평가하는데 이용한다.</p>

<ul>
  <li>K-분할 교차검증
    <ul>
      <li>데이터를 K개로 나눈 뒤 K-1개를 학습 데이터로, 남은 1개를 검증 데이터로 이용하여 예측 성능을 측정한다.</li>
      <li>이 과정을 K번 반복 시행하여 예측 성능을 평균화한다.</li>
    </ul>
  </li>
  <li>Leave-one-out 교차검증
    <ul>
      <li>K-분할 교차검증에서 K=1인 경우에, 데이터 하나만을 골라 검증 데이터로, 나머지는 전부 학습 데이터로 삼는 방법</li>
      <li>이를 반복하여 모든 데이터가 한 번씩 검증 데이터가 되도록 하는 방법이다.</li>
    </ul>
  </li>
</ul>

<h4 id="정칙화-regularization">정칙화 Regularization</h4>

<p>과대적합이 일어나면, 학습 데이터의 무작위 데이터 퍼짐에도 모형이 적합해지고 말아 검증 데이터를 제대로 예측하지 못한다.
일반적으로 모형이 복잡할 수록 과대적합이 일어나기 쉬워, <strong>모형의 복잡도를 조정하는 요소를 손실함수에 도입하는 것</strong>.</p>

<h2 id="예측-성능-측정">예측 성능 측정</h2>

<h3 id="이진-클래스-분류">이진 클래스 분류</h3>

<p>반응 변수가 2개의 클래스 A, B일 때를 이진 클래스 분류라 한다.</p>

<ul>
  <li>ROC 곡선과 AUC - Receiver Operating Characteristic Curve: 이 역치가 작은 값에서 큰 값으로 점점 변할 때의 위양성률(1 - 특이도)을 가로축으로, 민감도를 세로축으로 하여 그려지는 곡선 - Area Under the Curve: ROC 곡선 결과를 하나의 숫자로서 나타낼 때 이용하는 ROC 곡선보다도 아랫부분의 넓이
<img src="/public/img/241110/ROC_AUC.png" alt="ROC Curve and AUC" /></li>
</ul>

<h3 id="회귀">회귀</h3>

<ul>
  <li>평균제곱오차 MSE, Mean Square Error</li>
  <li>제곱근평균제곱오차 RMSE, Root Means Square Error</li>
  <li>평균절대오차 MAE, Mean Absolute Error</li>
</ul>

<h4 id="로지스틱-회귀">로지스틱 회귀</h4>

<h4 id="결정-트리와-랜덤-포레스트">결정 트리와 랜덤 포레스트</h4>

<h4 id="서포트-벡터-머신-svm-support-vector-machine">서포트 벡터 머신 SVM, Support Vector Machine</h4>

<h4 id="신경망-neural-network">신경망 Neural Network</h4>

<p>선형분리가 가능할 때
<img src="/public/img/241110/LinearDistribution.png" alt="When Linear Separation is possible" /></p>

<p>선형분리가 불가능할 때
<img src="/public/img/241110/NonLinearDistribution.png" alt="When Linear Separation is impossible" /></p>
:ET