---
layout: post
title: "[DB & RS] Week 6"
date: 2024-11-10
category: [Data Business & Recommendation System]
---

> 📌통계 101 X 데이터 분석 **Chapter 12-13**

### 차원 축소 Dimension Reduction

데이터의 특징을 유지하면서 분석이나 결과 해석에 도움을 주면서, 변수의 수를 줄이는 것을 **차원 축소**라 한다.

> ❗변수의 수를 줄이는 이유
>
> 1. 고차원 데이터 해석의 어려움이 있다.
> 2. 다중회귀분석에서는 설명변수끼리 강한 상관이 있는 상황을 다중공선성이 있다고 하며, 회귀 계수 추정이 불안정해지는 문제가 발생한다.
> 3. 차원의 저주

- 회귀분석에서 변수가 많다는 것은 그 만큼 파라미터 수가 많아지는 것을 의미한다.
- 표본크기 n이 충분하지 않은 상황이라면, 회귀계수를 올바르게 추정할 수 없는 문제가 생긴다.

## 주성분 분석

**PCA, Principal Component Analysis**

주성분 분석에서는 새로운 축을 설정하고, 그 축 위의 값으로 데이터를 새롭게 바라본다.

💡 **새로운 축은 데이터의 퍼짐이 가장 커지는 방향**으로!

- 첫 번째 새로운 축을 PC1(제1주성분)
- PC1과 **수직 방향**이고 데이터 퍼짐이 가장 커지는 방향으로, 두 번째 축인 PC2(제2주성분)를 설정
  - PC2에는 PC1으로 나타낼 수 없는 나머지 소량의 정보가 포함되어 있다.

변수의 수가 M일 때, **주성분의 개수는 최대 M**이다.
각 주성분이 가진 정보(분산)의 비율을 **기여율**이라 하고, '제1부터 제k주성분까지 전체 정보의 몇 %가 포함되는지'를 **누적기여율**

새수직이라는 성질을 가지기에 새로운 축의 변수와는 무상관.
각 주성분이 어떤 축인지를 조사하기 위해 각 주성분의 값과 원래 각 변수의 상관관계를 계산.

### 통계학과 기계학습의 차이

|             | 통계학                                  | 기계학습                 |
| ----------- | --------------------------------------- | ------------------------ |
| 데이터 크기 | 작은 표본 크기를 가진 데이터를 대상으로 | 대량의 데이터를 대상으로 |
| 경향        | 설명이나 해석을 중요시                  | 예측을 중요시            |

**통계학**의 선형회귀에서는 절편과 기울기를 가진 **단순한** 모형을 이용하며, 기울기는 설명변수 1개만이 달라졌을 때 반응변수가 어떻게 변화하는지를 나타내므로 쉽게 해석 가능하다.
**기계학습** 모형은 **복잡**하기(비선형이며 파라미터가 많기) 때문에, 보통 모형을 해석하기 어렵다. 복잡한 모형은 단순한 모형보다 표현력이 강하며, 복잡한 현상에 대한 예측 성능이 높다.

## 비지도 학습

정답 데이터가 없으며, 데이터의 배후에 있는 구조를 올바르게 추출하려는 목적으로 사용된다. 비슷한 데이터 집합을 발견하는 *군집분석*과 고차원 데이터를 적은 수의 차원으로 줄이는 *차원 축소*가 대표적이다.
