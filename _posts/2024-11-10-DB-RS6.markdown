---
layout: post
title: "[DB & RS] Week 6"
date: 2024-11-10
category: [Data Business & Recommendation System]
---

> 📌통계 101 X 데이터 분석 **Chapter 12-13**

### 차원 축소 Dimension Reduction

데이터의 특징을 유지하면서 분석이나 결과 해석에 도움을 주면서, 변수의 수를 줄이는 것을 **차원 축소**라 한다.

> ❗변수의 수를 줄이는 이유
>
> 1. 고차원 데이터 해석의 어려움이 있다.
> 2. 다중회귀분석에서는 설명변수끼리 강한 상관이 있는 상황을 다중공선성이 있다고 하며, 회귀 계수 추정이 불안정해지는 문제가 발생한다.
> 3. 차원의 저주

- 회귀분석에서 변수가 많다는 것은 그 만큼 파라미터 수가 많아지는 것을 의미한다.
- 표본크기 n이 충분하지 않은 상황이라면, 회귀계수를 올바르게 추정할 수 없는 문제가 생긴다.

## 주성분 분석

**PCA, Principal Component Analysis**

주성분 분석에서는 새로운 축을 설정하고, 그 축 위의 값으로 데이터를 새롭게 바라본다.

💡 **새로운 축은 데이터의 퍼짐이 가장 커지는 방향**으로!

- 첫 번째 새로운 축을 PC1(제1주성분)
- PC1과 **수직 방향**이고 데이터 퍼짐이 가장 커지는 방향으로, 두 번째 축인 PC2(제2주성분)를 설정
  - PC2에는 PC1으로 나타낼 수 없는 나머지 소량의 정보가 포함되어 있다.

변수의 수가 M일 때, **주성분의 개수는 최대 M**이다.
각 주성분이 가진 정보(분산)의 비율을 **기여율**이라 하고, '제1부터 제k주성분까지 전체 정보의 몇 %가 포함되는지'를 **누적기여율**

새수직이라는 성질을 가지기에 새로운 축의 변수와는 무상관.
각 주성분이 어떤 축인지를 조사하기 위해 각 주성분의 값과 원래 각 변수의 상관관계를 계산.

### 통계학과 기계학습의 차이

|             | 통계학                                  | 기계학습                 |
| ----------- | --------------------------------------- | ------------------------ |
| 데이터 크기 | 작은 표본 크기를 가진 데이터를 대상으로 | 대량의 데이터를 대상으로 |
| 경향        | 설명이나 해석을 중요시                  | 예측을 중요시            |

**통계학**의 선형회귀에서는 절편과 기울기를 가진 **단순한** 모형을 이용하며, 기울기는 설명변수 1개만이 달라졌을 때 반응변수가 어떻게 변화하는지를 나타내므로 쉽게 해석 가능하다.

**기계학습** 모형은 **복잡**하기(비선형이며 파라미터가 많기) 때문에, 보통 모형을 해석하기 어렵다. 복잡한 모형은 단순한 모형보다 표현력이 강하며, 복잡한 현상에 대한 예측 성능이 높다.

## 비지도 학습

**정답 데이터가 없으며**, 데이터의 배후에 있는 구조를 올바르게 추출하려는 목적으로 사용된다. 비슷한 데이터 집합을 발견하는 **_군집분석_**과 고차원 데이터를 적은 수의 차원으로 줄이는 **_차원 축소_**가 대표적이다.

### 군집분석 Cluster Analysis

각 데이터가 어떤 군집에 속하는지를 구하는 방법

- 데이터 간 유사도를 측정하고자 데이터 사이의 거리를 계산하지만, 거리의 정의는 다양하기에 분석하는 사람이 직접 지정해야 한다.
  - 결과에는 분석하는 사람의 자의성이 개입된다는 사실에 주의해야 한다.

#### k-평균 k-means

기본적인 군집화 방법으로, 몇 개의 군집으로 나눌 것인지 군집 개수 k를 정하며 시작한다.
각 데이터에 무작위로 군집을 할당하고, 각 군집의 중심 위치(데이터의 평균)를 구한다. 그 이후, 데이터마다 가장 근접한 군집을 할당하고 중심 위치를 다시 계산한다.

→ 이 과정을 반복해나가다가, 군집 할당이 바뀌지 않거나 할당의 변화량이 일정 이하일 때 계산을 멈추고, **각 데이터에 대한 군집 할당**을 얻는다.

#### 계층적 군집화 Hierarchical Clustering

모든 데이터가 다른 군집에 속하는 상태에서 시작하여, 가장 거리가 가까운 군집끼리 순서대로 합쳐 나간다.

<img src='/assets/images/241110/HClustering.png' alt='Hierarchical Clustering'>

**Dendrogram 덴드로그램**은 군집 개수는 미리 정해지지 않으며, 어느 단계에서 바라보느냐에 따라 달라진다.

**t-SNE(t-distributed Stochastic Neighbor Embedding)**는 복잡한 비선형관계에도 적용할 수 있는 방법이다.

## 지도 학습

특정 x값일 때, y가 어떤 값이 되는지에 관한 데이터를 바탕으로 그 관계를 학습해 나가는 방법이다.

> 📌 **학습**은 모형에서 얻을 수 있는 출력을 가능한 한 실제 y에 가깝도록 하고자 모형의 파라미터를 update하는 것.

계산 방법은 모형의 출력과 실제 y의 차이를 손실함수로 정식화하고, 이를 최소화하는 파라미터를 구하는 것.

> ❗반응변수 y가 양적 변수일 때는 **회귀**, 질적 변수일 때는 **분류**라 한다.

#### 예측

학습에 사용한 데이터(훈련 데이터)를 설명 및 예측하는 것이 아니라, 동일 조건에서 얻을 수 있는 미지의 데이터(검증 데이터)에 대해 설명 변수 x로 반응변수 y를 예측하는 것이다.

#### 교차검증 cross validation

동일 조건에서 얻은 미지의 데이터를 얼마나 잘 예측할 수 있는가를 검증하기 위해 실제로 얻은 데이터를 둘로 분할한다.
한 쪽 데이터를 사용해 학습을 진행하고, 나머지 한 쪽 데이터는 예측이 얼마나 좋은지 평가하는데 이용한다.

- K-분할 교차검증
  - 데이터를 K개로 나눈 뒤 K-1개를 학습 데이터로, 남은 1개를 검증 데이터로 이용하여 예측 성능을 측정한다.
  - 이 과정을 K번 반복 시행하여 예측 성능을 평균화한다.
- Leave-one-out 교차검증
  - K-분할 교차검증에서 K=1인 경우에, 데이터 하나만을 골라 검증 데이터로, 나머지는 전부 학습 데이터로 삼는 방법
  - 이를 반복하여 모든 데이터가 한 번씩 검증 데이터가 되도록 하는 방법이다.

#### 정칙화 Regularization

과대적합이 일어나면, 학습 데이터의 무작위 데이터 퍼짐에도 모형이 적합해지고 말아 검증 데이터를 제대로 예측하지 못한다.
일반적으로 모형이 복잡할 수록 과대적합이 일어나기 쉬워, **모형의 복잡도를 조정하는 요소를 손실함수에 도입하는 것**.

## 예측 성능 측정

### 이진 클래스 분류

반응 변수가 2개의 클래스 A, B일 때를 이진 클래스 분류라 한다.

- ROC 곡선과 AUC - Receiver Operating Characteristic Curve: 이 역치가 작은 값에서 큰 값으로 점점 변할 때의 위양성률(1 - 특이도)을 가로축으로, 민감도를 세로축으로 하여 그려지는 곡선 - Area Under the Curve: ROC 곡선 결과를 하나의 숫자로서 나타낼 때 이용하는 ROC 곡선보다도 아랫부분의 넓이
  <img src='/assets/images/241110/ROC_AUC.png' alt='ROC Curve and AUC'>

### 회귀

- 평균제곱오차 MSE, Mean Square Error
- 제곱근평균제곱오차 RMSE, Root Means Square Error
- 평균절대오차 MAE, Mean Absolute Error

#### 로지스틱 회귀

#### 결정 트리와 랜덤 포레스트

#### 서포트 벡터 머신 SVM, Support Vector Machine

#### 신경망 Neural Network

선형분리가 가능할 때
<img src='/assets/images/241110/LinearDistribution.png' alt='When Linear Separation is possible'>

선형분리가 불가능할 때
<img src='/assets/images/241110/NonLinearDistribution.png' alt='When Linear Separation is impossible'>
